# Awesome-LLM-Benchmark
Awesome-LLM-Benchmark: List of benchmarks for Large-Language Models


## Training and Evaluation Datasets of LLMs

## LLM Datasets/Benchmarks by Category

**Sentiment Analysis (Single-Sentence Tasks)**

- SST-2 (Stanford Sentiment Treebank)
- IMDb (Large Movie Review Dataset)

**Linguistic Acceptability (Single-Sentence Tasks)**

- CoLA (Corpus of Linguistic Acceptability)

**Simliarity and Paraphrase Tasks**

- MRPC (Microsoft Research Paraphrase Corpus)
- QQP (Quora Question Pairs)
- STS-B (Semantic Textual Similarity Benchmark)

**Natural Language Inference (NLI)**

- CB (CommitmentBank)
- RTE (Recognizing Textual Entailment)
- HellaSwag
- MNLI (Multi-Genre Natural Language Inference Corpus)
- RTE (Recognizing Textual Entailment)
- WNLI (Winograd NLI)
- Story Cloze
- SNLI (Stanford Natural Language Inference)
- ANLI (Adversarial NLI)
- SciTail
- AX
- SWAG (Situations With Adversarial Generations)

**Word Sense Disambiguation (WSD)**

- WiC (Word-in-Context)

**Coreference Resolution (coref.)**

- WSC (Winograd Schema Challenge)
- Winogrande

**Prediction**

- LAMBADA (LAnguage Modeling Broadened to Account for Discourse Aspects)

**Reading Comprehension (QA)**

- BoolQ (Boolean Questions)
- COPA (Choice of Plausible Alternatives)
- PIQA (Physical Interaction QA)
- QNLI (Stanford Question Answering Dataset)
- SQuAD (Stanford Question Answering Dataset)
- MultiRC (Multi-Sentence Reading Comprehension)
- ReCoRD (Reading Comprehension with Commonsense Reasoning Dataset)
- RACE (ReAding Comprehension dataset from Examinations)
- TriviaQA
- CoQA (Conversational Question Answering)
- MultiRC (Multi-Sentence Reading Comprehension)
- NaturalQA
- DROP (Discrete Reasoning Over the content of Paragraphs)
- OpenBookQA
- MBPP (Mostly Basic Programming Problems)
- QuAC (Question Answering in Context)
- Humanities
- Social Science
- STEM (Science, Technology, Engineering, and Mathmatics)

**IQ Test**

- ARC (AI2's Reasoning Challenge)

**Summarization**

- ROUGE (Recall-Oriented Understudy for Gisting Evaluation)
- XSum (Extreme Summarization)
- CNN/DM
- WikiLingua
- XL-Sum

**Functional Correctness for Sunthesizing**

- HumanEval

**Information Extraction**

- NER (Named Entity Recognition)
- FEVER (Fact Extraction and VERification)

**Math**

- GSM8K (Grade School Math 8.5K)
- MATH (Mathematical problem-solving ability)

## LLM Datasets/Benchmarks List

## Support for LLM Datasets/Benchmarks in Deep Learning Framewors
